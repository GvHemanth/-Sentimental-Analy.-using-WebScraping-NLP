{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6420039c",
   "metadata": {},
   "source": [
    " \n",
    "\n",
    "# <p style=\"text-align: center;\">Sentimental Analysis  </p>\n",
    "\n",
    "                                                             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a83962",
   "metadata": {},
   "source": [
    "### Importing required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a0ffdd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen as uReq\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010ca50f",
   "metadata": {},
   "source": [
    "### Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e5a8c721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-in-healthc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>https://insights.blackcoffer.com/what-if-the-c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39</td>\n",
       "      <td>https://insights.blackcoffer.com/what-jobs-wil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>https://insights.blackcoffer.com/will-machine-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>https://insights.blackcoffer.com/will-ai-repla...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   URL_ID                                                URL\n",
       "0      37  https://insights.blackcoffer.com/ai-in-healthc...\n",
       "1      38  https://insights.blackcoffer.com/what-if-the-c...\n",
       "2      39  https://insights.blackcoffer.com/what-jobs-wil...\n",
       "3      40  https://insights.blackcoffer.com/will-machine-...\n",
       "4      41  https://insights.blackcoffer.com/will-ai-repla..."
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing data file and checking the links\n",
    "import pandas as pd\n",
    "link=pd.read_excel(r\"C:\\Users\\heman\\OneDrive\\Black coffer\\Input.xlsx\")\n",
    "link['URL_ID']=link.URL_ID.astype(int)\n",
    "link.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "28f0c7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading each link and webscraping the relevant data and saving it in text file with URL_ID as file name\n",
    "\n",
    "ID=link.URL_ID[0]\n",
    "for blog in link.URL[:]:\n",
    "        uClient=uReq(blog)\n",
    "        page_html=uClient.read()\n",
    "        uClient.close()\n",
    "        page_soup=soup(page_html,\"html.parser\")                        #making html readable using beautiful soup\n",
    "        title=page_soup.find(\"h1\",attrs={\"class\":\"entry-title\"}).text  #extracting title\n",
    "        containers=page_soup.findAll(\"p\")                              #Finding all containers with p in html\n",
    "        paragraphs=[]\n",
    "        for paragraph in containers:\n",
    "               paragraphs.append(paragraph.get_text(' ',strip=True))   #Reading each paragraph in the link and appending it\n",
    "        text=[x.lower() for x in paragraphs]        \n",
    "        text=\" \".join(text)                                            #Changing from list to string\n",
    "        text=re.sub('[;:,\\?\\\"\\'\\/]','',text)   #Removing all punctuations except ! as this is a sentiment analysis.\n",
    "        bodytext=title.lower() +\" \"+ text       #Changing all to lower case\n",
    "        \n",
    "        text_link = open(f'{str(ID)}.txt',mode=\"w\",encoding='utf-8')   #Creating a text file with URLID as file name\n",
    "        text_link.write(bodytext)               #wrting the text to text file\n",
    "        text_link.close()\n",
    "        ID+=1              "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f4a201",
   "metadata": {},
   "source": [
    "### Stop Words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bd633788",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing glob and reading all the stop words text files\n",
    "\n",
    "import glob\n",
    "stop_words_path=glob.glob(r\"C:\\Users\\heman\\OneDrive\\Black coffer\\StopWords\\*.txt\") #searches all the txt files in stopwords folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6148a094",
   "metadata": {},
   "outputs": [],
   "source": [
    "#appending all stopwords into one list\n",
    "\n",
    "all_stop_words=[]\n",
    "\n",
    "for i in stop_words_path:\n",
    "            \n",
    "            stop=open(f'{i}',\"r\")\n",
    "            stop_list=stop.read().replace(\"\\n\",\",\").split(\",\")\n",
    "            all_stop_words.extend(stop_list)\n",
    "            stop.close()         \n",
    "        \n",
    "#Converting all stop words to small case       \n",
    "all_stop_words=[x.lower() for x in all_stop_words]       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2e13fe6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning the stop words as they contain \"|\"\n",
    "\n",
    "stripped_stop_words=[]\n",
    "for i in all_stop_words:\n",
    "    regex=re.search('(\\w*\\'*\\w*)(\\s*)(\\|*\\s*.*)',i)   #making a regex pattern so the words with | are stripped\n",
    "    stripped_stop_words.append(regex.group(1)) \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "13e6b0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Personal pronouns\n",
    "\n",
    "def personal_pronouns(split_body):\n",
    "    pronouns_count=0\n",
    "    for word in split_body:               #we count the no_of_pronouns in text which we took from before removing stopwords\n",
    "        pronouns=['^(i?)(i?)$','^we$','^my$','^ours$','^us$']\n",
    "        for pattern in pronouns:\n",
    "            if re.search(pattern,word,re.M):\n",
    "                pronouns_count+=1\n",
    "    return pronouns_count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5f3a33",
   "metadata": {},
   "source": [
    "### Cleaning using stop words lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "63a85525",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning the links with all the stop words\n",
    "pro=[]\n",
    "\n",
    "ID=link.URL_ID[0]\n",
    "\n",
    "for i in range(37,148):\n",
    "                                         #reading each text file\n",
    "    body=open(f'C:\\\\Users\\\\heman\\\\OneDrive\\\\Black coffer\\\\{i}.txt',\"r\",encoding='utf-8')\n",
    "    split_body=body.read().split()                             #converting it into list\n",
    "    pro_count=personal_pronouns(split_body)\n",
    "    pro.append(pro_count)\n",
    "    body.close()\n",
    "    clean_words=[word for word in split_body if word not in stripped_stop_words]  #cleaning the list by removing stop words\n",
    "    clean_txt=\" \".join(clean_words)                            #converting the list into string\n",
    "    clean_link=open(f'{ID}.txt','w',encoding='utf-8')                           #rewriting the text file after removing stop words\n",
    "    clean_link.write(clean_txt)\n",
    "    clean_link.close()                                         #closing the text file\n",
    "    ID+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3636b614",
   "metadata": {},
   "source": [
    "### Dictionary of Positive and Negative words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6598a716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Master Dictionary of positive words\n",
    "\n",
    "positive_words=set()\n",
    "positive_open=open(r\"C:\\Users\\heman\\OneDrive\\Black coffer\\MasterDictionary\\positive-words.txt\",'r')\n",
    "positive_list=positive_open.read().replace(\"\\n\",\",\").split(\",\")  #reading positive words text file and converting to list\n",
    "for i in positive_list:\n",
    "        positive_words.add(i)                                    #adding each word in list to dictionary\n",
    "positive_words.remove(\"\")        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "41afe57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Master Dictionary of negative words\n",
    "\n",
    "negative_words=set()\n",
    "negative_open=open(r\"C:\\Users\\heman\\OneDrive\\Black coffer\\MasterDictionary\\negative-words.txt\",'r')\n",
    "negative_list=negative_open.read().replace(\"\\n\",\",\").split(\",\")  #reading negative words text file and converting to list\n",
    "for i in negative_list:\n",
    "        negative_words.add(i)                                    #adding each word in list to dictionary\n",
    "negative_words.remove(\"\")  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac97530",
   "metadata": {},
   "source": [
    "### Extracting derived variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0f63bd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#positive_score\n",
    "\n",
    "def positive_score(word_tokens):                   #calculating positive score if word is found in positive dictionary\n",
    "        pos_score=0\n",
    "        for word in word_tokens:\n",
    "            if word in positive_words:\n",
    "                pos_score+=1                       #adding by +1 if word found in positive dictionary\n",
    "        return pos_score\n",
    "        \n",
    "#negative_score\n",
    "\n",
    "def negative_score(word_tokens):                  #calculating negative score if word is found in negative dictionary\n",
    "        neg_score=0\n",
    "        for word in word_tokens:\n",
    "            if word in negative_words:\n",
    "                neg_score-=1\n",
    "        neg_score*=-1                            #multiplying by -1 to make the score a positive value\n",
    "        return neg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "88d298f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polarity score\n",
    "\n",
    "def polarity_score(word_tokens):      #calculating polarity score to check if text is positive or negative in nature\n",
    "        pos_score=0\n",
    "        for word in word_tokens:\n",
    "            if word in positive_words:\n",
    "                pos_score+=1\n",
    "        neg_score=0\n",
    "        for word in word_tokens:\n",
    "            if word in negative_words:\n",
    "                neg_score-=1\n",
    "        neg_score*=-1          \n",
    "        polarity=round((pos_score-neg_score)/((pos_score+neg_score)+0.000001),3)\n",
    "        return polarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "894973e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subjectivity score\n",
    "\n",
    "def subjectivity_score(word_tokens):#calculating subjectivity score by using positive and negative score\n",
    "        pos_score=0\n",
    "        for word in word_tokens:\n",
    "            if word in positive_words:\n",
    "                pos_score+=1\n",
    "        neg_score=0\n",
    "        for word in word_tokens:\n",
    "            if word in negative_words:\n",
    "                neg_score-=1\n",
    "        neg_score*=-1      \n",
    "        \n",
    "        subj_score=round((pos_score+neg_score)/((len(word_tokens))+0.000001),3)\n",
    "        return subj_score\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf50fd15",
   "metadata": {},
   "source": [
    "### Analysis of readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b993753c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of readability\n",
    "\n",
    "##Average sentence length\n",
    "def average_sentence_length(word_tokens,sent_tokens):     # checking the average sentence length\n",
    "            avg_sent_len=round(len(word_tokens)/len(sent_tokens),3)\n",
    "            return avg_sent_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0cf8dca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#syllable words count\n",
    "\n",
    "def syllables_count(word):\n",
    "    count = 0\n",
    "    complex_count=0\n",
    "    vowels = \"aeiouy\"                             #assigning all vowels and letter y\n",
    "    for token in word:\n",
    "        if token[0] in vowels:                    #reading each word in the list\n",
    "            count += 1\n",
    "        for index in range(1, len(token)):\n",
    "            if token[index] in vowels and token[index - 1] not in vowels:   #checking if no two same vowels are consecutive\n",
    "                count += 1\n",
    "        if token.endswith(\"ed\"or\"es\"):           #reducing the count by 1 if words end with 'ed' and 'es'\n",
    "            count -= 1\n",
    "        if count == 0:\n",
    "            count += 1\n",
    "    return count                                 #returning the count of syllables\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c79ddbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Complex words count\n",
    "\n",
    "def complex_words_count(word):                  #counting no:of complex words \n",
    "    complex_word_count=0\n",
    "    vowels = \"aeiouy\"\n",
    "    for token in word:\n",
    "        count=0\n",
    "        if token[0] in vowels:\n",
    "            count += 1\n",
    "        for index in range(1, len(token)):\n",
    "            if token[index] in vowels and token[index - 1] not in vowels:\n",
    "                count += 1\n",
    "        if token.endswith(\"ed\"or\"es\"):\n",
    "            count -= 1\n",
    "        if count == 0:\n",
    "            count += 1\n",
    "           \n",
    "        if count > 2: \n",
    "            complex_word_count+=1               #adding by 1 to count if no:of syllables in each word are more than 2\n",
    "    return complex_word_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d2a5647d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Percentage of complex words\n",
    "\n",
    "def perc_complex_words(word_tokens):\n",
    "            perc_compl=round(complex_words_count(word_tokens)/len(word_tokens),3)  #calc percentage of complex words\n",
    "            return perc_compl\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a5ce0445",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gunning Fog index\n",
    "\n",
    "def fog_index(word_tokens,sent_tokens):          #calculating gunning fog index\n",
    "    index=round(0.4*(average_sentence_length(word_tokens,sent_tokens)+perc_complex_words(word_tokens)),3)\n",
    "    return index\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd18d95",
   "metadata": {},
   "source": [
    "### Average number of words per sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9d4bc976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avg number of words per sentence\n",
    "\n",
    "def avg_words_per_sentence(word_tokens,sent_tokens):\n",
    "            avg_words=round(len(word_tokens)/len(sent_tokens),3)   #calculating avg number of words in the sentence\n",
    "            return avg_words\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85836fa8",
   "metadata": {},
   "source": [
    "### Word Count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e6acac7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#counting the no:of words left after remving stop words using nltk package\n",
    "\n",
    "def cleaned_text_extended(word_tokens):\n",
    "    from nltk.corpus import stopwords\n",
    "    cleaned_word_extended=[word for word in word_tokens if word not in stopwords.words('english')]  #removing more stopwords\n",
    "    punctuations='.!?,'\n",
    "    cleaned_word_extended=[symbol for symbol in cleaned_word_extended if symbol not in punctuations] #removing punctuations\n",
    "    return cleaned_word_extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4f23d2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#counting no:of words in text after remving all stop words from nltk stopwords\n",
    "\n",
    "def cleaned_nltk(word_tokens):\n",
    "    count=len(cleaned_text_extended(word_tokens))\n",
    "    return count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14ea01b",
   "metadata": {},
   "source": [
    "### Average word length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d559836f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average word length\n",
    "\n",
    "#we take the text from cleaned word extended\n",
    "def avg_word_length(word_tokens):\n",
    "    word_length=0\n",
    "    for word in cleaned_text_extended(word_tokens):       #Iterating against each word and finding its length\n",
    "        word_length+=len(word)\n",
    "\n",
    "    avg_word_length=round(word_length/len(cleaned_text_extended(word_tokens)),3)  #calculating the average word length\n",
    "    return avg_word_length\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86855fa8",
   "metadata": {},
   "source": [
    "### Reading output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8a147793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYLLABLE PER WORD</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-in-healthc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>https://insights.blackcoffer.com/what-if-the-c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39</td>\n",
       "      <td>https://insights.blackcoffer.com/what-jobs-wil...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>https://insights.blackcoffer.com/will-machine-...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>https://insights.blackcoffer.com/will-ai-repla...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   URL_ID                                                URL  POSITIVE SCORE  \\\n",
       "0      37  https://insights.blackcoffer.com/ai-in-healthc...             NaN   \n",
       "1      38  https://insights.blackcoffer.com/what-if-the-c...             NaN   \n",
       "2      39  https://insights.blackcoffer.com/what-jobs-wil...             NaN   \n",
       "3      40  https://insights.blackcoffer.com/will-machine-...             NaN   \n",
       "4      41  https://insights.blackcoffer.com/will-ai-repla...             NaN   \n",
       "\n",
       "   NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  AVG SENTENCE LENGTH  \\\n",
       "0             NaN             NaN                 NaN                  NaN   \n",
       "1             NaN             NaN                 NaN                  NaN   \n",
       "2             NaN             NaN                 NaN                  NaN   \n",
       "3             NaN             NaN                 NaN                  NaN   \n",
       "4             NaN             NaN                 NaN                  NaN   \n",
       "\n",
       "   PERCENTAGE OF COMPLEX WORDS  FOG INDEX  AVG NUMBER OF WORDS PER SENTENCE  \\\n",
       "0                          NaN        NaN                               NaN   \n",
       "1                          NaN        NaN                               NaN   \n",
       "2                          NaN        NaN                               NaN   \n",
       "3                          NaN        NaN                               NaN   \n",
       "4                          NaN        NaN                               NaN   \n",
       "\n",
       "   COMPLEX WORD COUNT  WORD COUNT  SYLLABLE PER WORD  PERSONAL PRONOUNS  \\\n",
       "0                 NaN         NaN                NaN                NaN   \n",
       "1                 NaN         NaN                NaN                NaN   \n",
       "2                 NaN         NaN                NaN                NaN   \n",
       "3                 NaN         NaN                NaN                NaN   \n",
       "4                 NaN         NaN                NaN                NaN   \n",
       "\n",
       "   AVG WORD LENGTH  \n",
       "0              NaN  \n",
       "1              NaN  \n",
       "2              NaN  \n",
       "3              NaN  \n",
       "4              NaN  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output=pd.read_excel(r\"C:\\Users\\heman\\OneDrive\\Black coffer\\Output Data Structure.xlsx\")\n",
    "output.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f118e77",
   "metadata": {},
   "source": [
    "### Assigning values to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1e3e0001",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize                 #importing word tokenizer\n",
    "from nltk.tokenize import sent_tokenize                 #importing sentence tokenizer\n",
    "\n",
    "row_index=0\n",
    "                                                          #reading all text files in the folder \n",
    "for file in range(37,148):\n",
    "    clean_text=open(f'C:\\\\Users\\\\heman\\\\OneDrive\\\\Black coffer\\\\{file}.txt',\"r\",encoding='utf-8')\n",
    "    word_tokens=word_tokenize(clean_text.read())        #reading the txt file and storing word tokens as a list\n",
    "    clean_text.close()\n",
    "    clean_text=open(f'C:\\\\Users\\\\heman\\\\OneDrive\\\\Black coffer\\\\{file}.txt',\"r\",encoding='utf-8')\n",
    "    sent_tokens=sent_tokenize(clean_text.read())        #reading the text file and storing sentence tokens as a list\n",
    "    clean_text.close()\n",
    "   \n",
    "    positive_final=positive_score(word_tokens)          #checking the positive score\n",
    "    \n",
    "    \n",
    "    negative_final=negative_score(word_tokens)          #checking the negative score\n",
    "   \n",
    "    \n",
    "    polarity_final=polarity_score(word_tokens)  #checking the polarity score\n",
    "   \n",
    "    \n",
    "    subject_final=subjectivity_score(word_tokens)    #Checking the subjectivity score\n",
    "    \n",
    "    \n",
    "    avg_sent_len_final=average_sentence_length(word_tokens,sent_tokens)  #checking the avg length of sentence\n",
    "   \n",
    "    \n",
    "    syllables_final=syllables_count(word_tokens)        #checking the no:of syllables\n",
    "   \n",
    "    \n",
    "    complex_final=complex_words_count(word_tokens)      #checking the no:of complex words\n",
    "   \n",
    "    \n",
    "    perc_complex_final=perc_complex_words(word_tokens)  #checking percentage of complex words\n",
    "   \n",
    "    \n",
    "    fog_index_final=fog_index(word_tokens,sent_tokens)  #checking the gunning fox inex\n",
    "   \n",
    "    \n",
    "    avg_words_sent_final=avg_words_per_sentence(word_tokens,sent_tokens)  #checking no:of avg words in a sentence\n",
    "   \n",
    "    \n",
    "    clean_text_len_final=cleaned_nltk(word_tokens)#checking the length after cleaning using nltk stop words\n",
    "   \n",
    "    \n",
    "    pronouns_final=personal_pronouns(word_tokens)\n",
    "   \n",
    "    \n",
    "    avg_word_len_final=avg_word_length(word_tokens)     #checking the avg word length \n",
    "   \n",
    "    \n",
    "    #Assigning the claculated values to the dataframe\n",
    "    \n",
    "    output.loc[row_index] = pd.Series({'URL_ID':output.URL_ID[row_index],'URL':output.URL[row_index],\n",
    "                                       'POSITIVE SCORE':positive_final, 'NEGATIVE SCORE':negative_final,\n",
    "                                       'POLARITY SCORE':polarity_final,'SUBJECTIVITY SCORE':subject_final,\n",
    "                                       'AVG SENTENCE LENGTH':avg_sent_len_final,'PERCENTAGE OF COMPLEX WORDS':perc_complex_final,\n",
    "                                       'FOG INDEX':fog_index_final,'AVG NUMBER OF WORDS PER SENTENCE':avg_words_sent_final,\n",
    "                                       'COMPLEX WORD COUNT':complex_final,'WORD COUNT':clean_text_len_final,\n",
    "                                       'SYLLABLE PER WORD':syllables_final,'PERSONAL PRONOUNS':pro[row_index],\n",
    "                                       'AVG WORD LENGTH':avg_word_len_final})\n",
    "    row_index+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c4dd4140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYLLABLE PER WORD</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>URL_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>https://insights.blackcoffer.com/ai-in-healthc...</td>\n",
       "      <td>64.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.089</td>\n",
       "      <td>14.554</td>\n",
       "      <td>0.449</td>\n",
       "      <td>6.001</td>\n",
       "      <td>14.554</td>\n",
       "      <td>484.0</td>\n",
       "      <td>992.0</td>\n",
       "      <td>2489.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>https://insights.blackcoffer.com/what-if-the-c...</td>\n",
       "      <td>58.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.139</td>\n",
       "      <td>10.162</td>\n",
       "      <td>0.315</td>\n",
       "      <td>4.191</td>\n",
       "      <td>10.162</td>\n",
       "      <td>218.0</td>\n",
       "      <td>579.0</td>\n",
       "      <td>1332.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>https://insights.blackcoffer.com/what-jobs-wil...</td>\n",
       "      <td>64.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.267</td>\n",
       "      <td>0.108</td>\n",
       "      <td>11.024</td>\n",
       "      <td>0.450</td>\n",
       "      <td>4.590</td>\n",
       "      <td>11.024</td>\n",
       "      <td>422.0</td>\n",
       "      <td>833.0</td>\n",
       "      <td>2181.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>https://insights.blackcoffer.com/will-machine-...</td>\n",
       "      <td>60.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.379</td>\n",
       "      <td>0.119</td>\n",
       "      <td>8.738</td>\n",
       "      <td>0.343</td>\n",
       "      <td>3.632</td>\n",
       "      <td>8.738</td>\n",
       "      <td>252.0</td>\n",
       "      <td>637.0</td>\n",
       "      <td>1516.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>6.959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>https://insights.blackcoffer.com/will-ai-repla...</td>\n",
       "      <td>56.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.418</td>\n",
       "      <td>0.087</td>\n",
       "      <td>12.667</td>\n",
       "      <td>0.355</td>\n",
       "      <td>5.209</td>\n",
       "      <td>12.667</td>\n",
       "      <td>324.0</td>\n",
       "      <td>797.0</td>\n",
       "      <td>1856.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6.907</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      URL  POSITIVE SCORE  \\\n",
       "URL_ID                                                                      \n",
       "37      https://insights.blackcoffer.com/ai-in-healthc...            64.0   \n",
       "38      https://insights.blackcoffer.com/what-if-the-c...            58.0   \n",
       "39      https://insights.blackcoffer.com/what-jobs-wil...            64.0   \n",
       "40      https://insights.blackcoffer.com/will-machine-...            60.0   \n",
       "41      https://insights.blackcoffer.com/will-ai-repla...            56.0   \n",
       "\n",
       "        NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  \\\n",
       "URL_ID                                                       \n",
       "37                32.0           0.333               0.089   \n",
       "38                38.0           0.208               0.139   \n",
       "39                37.0           0.267               0.108   \n",
       "40                27.0           0.379               0.119   \n",
       "41                23.0           0.418               0.087   \n",
       "\n",
       "        AVG SENTENCE LENGTH  PERCENTAGE OF COMPLEX WORDS  FOG INDEX  \\\n",
       "URL_ID                                                                \n",
       "37                   14.554                        0.449      6.001   \n",
       "38                   10.162                        0.315      4.191   \n",
       "39                   11.024                        0.450      4.590   \n",
       "40                    8.738                        0.343      3.632   \n",
       "41                   12.667                        0.355      5.209   \n",
       "\n",
       "        AVG NUMBER OF WORDS PER SENTENCE  COMPLEX WORD COUNT  WORD COUNT  \\\n",
       "URL_ID                                                                     \n",
       "37                                14.554               484.0       992.0   \n",
       "38                                10.162               218.0       579.0   \n",
       "39                                11.024               422.0       833.0   \n",
       "40                                 8.738               252.0       637.0   \n",
       "41                                12.667               324.0       797.0   \n",
       "\n",
       "        SYLLABLE PER WORD  PERSONAL PRONOUNS  AVG WORD LENGTH  \n",
       "URL_ID                                                         \n",
       "37                 2489.0                1.0            7.498  \n",
       "38                 1332.0                6.0            6.922  \n",
       "39                 2181.0                2.0            7.579  \n",
       "40                 1516.0               18.0            6.959  \n",
       "41                 1856.0               15.0            6.907  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking the data frame output\n",
    "output=output.set_index('URL_ID')\n",
    "output.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07c0832",
   "metadata": {},
   "source": [
    "### Exporting data frame to excel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "863ef8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported Successfully\n"
     ]
    }
   ],
   "source": [
    "#giving file name\n",
    "\n",
    "file_name='Output Data Structure.xlsx'     #giving a file name\n",
    "output.to_excel(file_name)                 #output to excel\n",
    "print(\"Exported Successfully\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
